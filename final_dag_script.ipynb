{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c85bf4b9",
   "metadata": {},
   "source": [
    "# NBA Analytics ETL / DAGs Overview\n",
    "\n",
    "## Project Purpose\n",
    "This project ingests, validates, enriches, and surfaces advanced NBA data for downstream analytics. Key inputs include NBA Stats API endpoints (Synergy play types, Player Estimated Metrics, roster/box-score), Basketball-Reference advanced stats, injury reports, Spotrac financial/contract data, and defensive metrics. The pipelines are built with Airflow TaskFlow and classic operator patterns, emphasize auditability, null-legitimacy validation, identity enrichment, incremental history preservation, and eventual analytical consumption via DuckDB/Parquet.\n",
    "\n",
    "## Primary Data Sources\n",
    "- **Synergy Play Types (NBA Stats API)**  \n",
    "  Granular offensive/defensive play-type metrics per player (e.g., Isolation, Transition, Spotup) with rate/volume stats. Includes null-legitimacy validation to distinguish absent play types from data loss. (`eda/synergy_playtype_pull.py`)\n",
    "- **Player Estimated Metrics (NBA Stats API)**  \n",
    "  Estimated advanced metrics per player-season (e.g., offensive/defensive ratings, usage, pace). Raw JSON + parameters + DataFrames are archived; enriched with canonical player identity. (`eda/nba_em_connector.py`)\n",
    "- **Roster & Box Score Data (nba_api_ingest)**  \n",
    "  Hourly ingestion of roster and box-score data to keep the lake near-real-time. Supports incremental pulls with fallback to full season scrape. (`dags/nba_api_ingest.py`)\n",
    "- **Advanced Season-Level Metrics (Basketball-Reference)**  \n",
    "  Daily scrape of season-level advanced stats (e.g., from Basketball-Reference), with seasonal completeness logic and fail-fast behavior when data should exist. (`dags/nba_advanced_ingest.py`)\n",
    "- **Injury Reports**  \n",
    "  Aggregates daily NBA injury report feeds (e.g., ESPN/statsurge) with incremental ingestion, cleansing, and historical merging. (`dags/new_injury_reports.py`, `eda/combining_injury_reports.py`)\n",
    "- **Spotrac Misc Assets**  \n",
    "  Salary cap history, extensions, tax tracker, transactions, multi-year cap/cash data. Schema drift detection, logical requirement resolution, and asset validation. (`dags/spotrac_misc_assets.py`)\n",
    "- **Defensive Metrics**  \n",
    "  Custom defensive metrics collection, season-aware, cached, and merged into a canonical dataset. (`dags/defensive_metrics_collect.py`)\n",
    "\n",
    "## High-Level Pipeline Components\n",
    "\n",
    "### Connectors / ETL Modules\n",
    "- `eda/synergy_playtype_pull.py`: \n",
    "  - Fetches Synergy play-type tables, pivots long ➜ wide, validates null legitimacy, merges offensive/defensive, and builds season/master wide tables.\n",
    "- `eda/nba_em_connector.py`: \n",
    "  - Fetches Player Estimated Metrics; archives raw, normalizes, enriches (player identity), computes null diagnostics, and builds historical master via smoke tests.\n",
    "- `eda/nba_basic_advanced_stats`: \n",
    "  - Scrapes advanced metrics per season for rate-based analytical enrichment. (`nba_advanced_ingest.py` wraps this.)\n",
    "- `eda/nba_api_ingest.py` logic: \n",
    "  - Ingests roster and box-score data hourly with incremental logic and fallbacks to full season pulls.\n",
    "- `eda/new_injury_reports.py` & `eda/combining_injury_reports.py`: \n",
    "  - Injury ingestion, normalization, deduplication, and historical merge.\n",
    "- `eda/spotrac_connector` (used by `spotrac_misc_assets.py`): \n",
    "  - Retrieves Spotrac financial/contract assets with validation helpers.\n",
    "- `eda/defensive_metrics.py` (used by `defensive_metrics_collect.py`): \n",
    "  - Collects defensive analytics and consolidates per-season outputs.\n",
    "\n",
    "### DAGs (Airflow)\n",
    "- `nba_player_estimated_metrics.py`  \n",
    "  Drives end-to-end for Player Estimated Metrics: determines missing seasons, fetches raw, normalizes, merges into a final parquet, and validates outputs.\n",
    "- `nba_api_ingest.py`  \n",
    "  Hourly ingestion of roster + box-score data from `nba_api`. Uses incremental pull logic to keep data fresh while avoiding redundant full scrapes; respects existing partitions and backfills as needed. (`dags/nba_api_ingest.py`)\n",
    "- `nba_advanced_ingest.py`  \n",
    "  Daily scrape of Basketball-Reference season-level advanced stats with logic to surface missing expected data during active windows (Nov–Jun). (`dags/nba_advanced_ingest.py`)\n",
    "- `nba_data_loader.py`  \n",
    "  Consumes upstream artifacts (player, advanced, injury), validates them, and loads into DuckDB. Builds materialized views joining player, advanced, and injury data for analytical querying. (`dags/nba_data_loader.py`)\n",
    "- `new_injury_reports.py`  \n",
    "  Dynamically maps over dates to pull, upsert, validate, and combine injury report datasets.\n",
    "- `spotrac_misc_assets.py`  \n",
    "  Fetches and validates Spotrac static and rolling assets; handles schema drift, merges yearly shards, and enforces logical column requirements.\n",
    "- `defensive_metrics_collect.py`  \n",
    "  Season-aware collection and incremental merge of defensive metrics with cache logic.\n",
    "\n",
    "## Directory Layout (key slices)\n",
    "\n",
    "api/src/airflow_project/\n",
    "├── dags/ # Airflow DAG definitions\n",
    "│ ├── nba_player_estimated_metrics.py\n",
    "│ ├── nba_api_ingest.py\n",
    "│ ├── nba_advanced_ingest.py\n",
    "│ ├── nba_data_loader.py\n",
    "│ ├── new_injury_reports.py\n",
    "│ ├── spotrac_misc_assets.py\n",
    "│ └── defensive_metrics_collect.py\n",
    "├── eda/ # Extract/Transform/Enrich modules\n",
    "│ ├── synergy_playtype_pull.py\n",
    "│ ├── nba_em_connector.py\n",
    "│ ├── nba_basic_advanced_stats/ # roster/box-score and advanced scrape logic\n",
    "│ └── ... # Other domain-specific ingestion logic\n",
    "├── data/\n",
    "│ ├── new_processed/ # Partitioned, near-real-time ingest (e.g., season=.../part.parquet)\n",
    "│ ├── raw/ # Raw archived JSON/Parquet (per-source)\n",
    "│ ├── silver/ # Normalized/enriched intermediate layers\n",
    "│ └── final/ # Canonical merged outputs for analytics\n",
    "├── utils/ # Shared helpers: storage, enrichment, config, incremental utils, validation\n",
    "└── config.py / constants # Paths, directories (e.g., FINAL_DATASET_DIR, DATA_DIR, INJURY_DIR)\n",
    "\n",
    "\n",
    "## Core Concepts & Guarantees\n",
    "- **Auditability:** Full JSON + request parameters are preserved for every API fetch.\n",
    "- **Null-legitimacy Validation:** Especially for Synergy play types—only legitimate absences are zero-filled; any data loss during pivot triggers an error with detailed diagnostics.\n",
    "- **Identity Enrichment:** Player (and optionally team) names/IDs are normalized via canonical joins (`enrich_join_by`) to ensure consistency across sources.\n",
    "- **Incremental History:** DAGs compare requested seasons/dates against existing cached outputs, process only missing slices, and merge new shards without duplicate reprocessing unless `force_full` is specified.\n",
    "- **Near Real-Time Ingestion:** Roster/box-score data is pulled hourly to keep the lake fresh; advanced metrics and other slower-moving data have daily or seasonal cadences.\n",
    "- **Schema Drift Detection:** Spotrac and other asset pipelines persist prior schema/stats to detect column additions/removals or mean drifts.\n",
    "- **Smoke Tests & Health Checks:** Critical endpoints (e.g., Player Estimated Metrics) have historical smoke tests to validate coverage and data integrity prior to publishing.\n",
    "\n",
    "## Typical Usage\n",
    "1. **Trigger DAGs** via Airflow (UI/CLI). Supply params such as seasons, dates, or `force_full`.\n",
    "2. **Monitor logs** for diagnostics: missing-season logic, enrichment summaries, null/coverage reports, schema drift alerts.\n",
    "3. **Consume final datasets** from `data/final/*.parquet` or query via DuckDB materialized views populated by `nba_data_loader`. Example view: `v_player_full_<season>` combines player, advanced, and injury data.\n",
    "\n",
    "## Validation & Health Checks\n",
    "- Each DAG includes a validation task asserting required fields, deduplication consistency, and basic sanity (e.g., rating top-n, row counts).\n",
    "- Enrichment stages log before/after null diagnostics and surface identity mismatches (e.g., missing team info).\n",
    "- Schema snapshots enable drift detection over time. \n",
    "- `nba_data_loader` ensures upstream dependencies complete via sensors before loading into DuckDB and builds analytical views.\n",
    "\n",
    "## Failure Modes / Remediation\n",
    "- **Stale/Missing API Data:** Missing-season tracking with `force_full` override allows re-fetching; smoke tests surface unexpected gaps.\n",
    "- **Illegitimate Nulls:** Pivot validation will raise with sampled context, prompting investigation of upstream transformation logic.\n",
    "- **Schema Drift:** Incoming column changes are detected and logged; update logical mappings accordingly.\n",
    "- **Enrichment Failures:** Logged explicitly with context; may need updates to canonical reference tables or fuzzy matching logic.\n",
    "\n",
    "## Running Locally\n",
    "- Install dependencies (`nba_api`, `pandas`, `airflow`, `duckdb`, `pendulum`, internal utils).\n",
    "- Ensure configuration (`utils.config`, `nba_basics_config`, etc.) points to correct root directories (e.g., `DATA_DIR`, `FINAL_DATASET_DIR`).\n",
    "- Kick off DAGs manually for desired season/date ranges.\n",
    "- Example: Run the Player Estimated Metrics smoke test:\n",
    "  ```python\n",
    "  from eda.nba_em_connector import _smoke_test_all_seasons\n",
    "  results = _smoke_test_all_seasons(start_season=\"2015-16\", end_season=\"2024-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e24025fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing api/src/airflow_project/utils/config.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/utils/config.py\n",
    "\"\"\"\n",
    "Central configuration for the NBA‑Player‑Valuation project.\n",
    "All magic values live here so they can be tweaked without code edits.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# ── Core directories ───────────────────────────────────────────────────────────\n",
    "def find_project_root(name: str = \"airflow_project\") -> Path:\n",
    "    \"\"\"\n",
    "    Walk up from this file (or cwd) until a directory named `name` is found.\n",
    "    Fallback to cwd if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = Path(__file__).resolve()\n",
    "    except NameError:\n",
    "        p = Path.cwd()\n",
    "    # walk through p and its parents\n",
    "    for parent in (p, *p.parents):\n",
    "        if parent.name == name or (parent / \".git\").is_dir():\n",
    "            return parent\n",
    "    # no match → fallback\n",
    "    return Path.cwd()\n",
    "\n",
    "# Allow explicit override\n",
    "if find_project_root():\n",
    "    PROJECT_ROOT = Path(find_project_root()).resolve() # / \"api/src/airflow_project\"\n",
    "else:\n",
    "    PROJECT_ROOT = find_project_root() # / \"api/src/airflow_project\"\n",
    "\n",
    "\n",
    "DATA_DIR: Path = Path(PROJECT_ROOT / \"data\")\n",
    "LOG_DIR: Path = Path(PROJECT_ROOT / \"logs\")\n",
    "DUCKDB_FILE: Path = Path(DATA_DIR / \"nba.duckdb\")\n",
    "\n",
    "# NBA stats API\n",
    "NBA_API_RPM: int = int(os.getenv(\"NBA_API_RPM\", \"12\"))  # requests per minute\n",
    "\n",
    "# Spotrac scraping\n",
    "SPOTRAC_BASE: str = \"https://www.spotrac.com/nba\"\n",
    "SPOTRAC_FREE_AGENTS: str = f\"{SPOTRAC_BASE}/free-agents/{{year}}/\"\n",
    "SPOTRAC_CAP_TRACKER: str = f\"{SPOTRAC_BASE}/cap/{{year}}/\"\n",
    "SPOTRAC_TAX_TRACKER: str = f\"{SPOTRAC_BASE}/tax/_/year/{{year}}/\"\n",
    "# Spotrac dedicated folder\n",
    "\n",
    "\n",
    "# Injury sources\n",
    "NBA_OFFICIAL_INJURY_URL: str = \"https://cdn.nba.com/static/json/injury/injury_report_{{date}}.json\"\n",
    "ROTOWIRE_RSS: str = \"https://www.rotowire.com/rss/news.php?sport=NBA\"\n",
    "\n",
    "# StatsD (optional)\n",
    "STATSD_HOST: str = os.getenv(\"STATSD_HOST\", \"localhost\")\n",
    "STATSD_PORT: int = int(os.getenv(\"STATSD_PORT\", \"8125\"))\n",
    "\n",
    "# ── Data ranges ────────────────────────────────────────────────────────────────\n",
    "SEASONS: range = range(2015, 2026)  # inclusive upper bound matches Spotrac sample\n",
    "\n",
    "# Thread pools & concurrency\n",
    "MAX_WORKERS: int = int(os.getenv(\"NPV_MAX_WORKERS\", \"8\")) \n",
    "\n",
    "\n",
    "# ── Core data directories ───────────────────────────────────────────────────────────\n",
    "RAW_DIR      : Path = DATA_DIR / \"raw\"\n",
    "DEBUG_DIR    : Path = DATA_DIR / \"debug\"\n",
    "EXPORTS_DIR  : Path = DATA_DIR / \"exports\"\n",
    "INJURY_DIR   : Path = DATA_DIR / \"injury_reports\"\n",
    "INJURY_DATASETS_DIR : Path = DATA_DIR / \"injury_datasets\"\n",
    "NBA_BASIC_ADVANCED_STATS_DIR : Path = DATA_DIR / \"nba_basic_advanced_stats\"\n",
    "ADVANCED_METRICS_DIR: Path = DATA_DIR / \"new_processed\" / \"advanced_metrics\"\n",
    "NBA_BASE_DATA_DIR : Path = DATA_DIR / \"nba_processed\"\n",
    "DEFENSE_DATA_DIR : Path = DATA_DIR / \"defense_metrics\"\n",
    "FINAL_DATASET_DIR : Path = DATA_DIR / \"merged_final_dataset\"\n",
    "PLAY_TYPES_DIR : Path = DATA_DIR / \"synergyplay_types\"\n",
    "SPOTRAC_DIR  : Path = DATA_DIR / \"spotrac_contract_data\"\n",
    "SILVER_DIR   : Path = SPOTRAC_DIR / \"silver\"\n",
    "FINAL_DIR    : Path = SPOTRAC_DIR / \"final\"\n",
    "SPOTRAC_DEBUG_DIR    : Path = SPOTRAC_DIR / \"debug\"\n",
    "SPOTRAC_RAW_DIR      : Path = SPOTRAC_DIR / \"raw\"\n",
    "\n",
    "# ── Helper functions (single source of truth) ────────────────────────────────\n",
    "def get_injury_base_dir() -> Path:\n",
    "    \"\"\"\n",
    "    Return the canonical injury_reports root.  \n",
    "    ENV `INJURY_DATA_DIR` wins; otherwise we fall back to INJURY_DIR.\n",
    "    Always creates the dir so callers can assume it exists.\n",
    "    \"\"\"\n",
    "    base = Path(os.getenv(\"INJURY_DATA_DIR\", INJURY_DIR)).resolve()\n",
    "    return base\n",
    "\n",
    "def injury_path(*parts: str) -> Path:\n",
    "    \"\"\"Shorthand for get_injury_base_dir().joinpath(*parts).resolve().\"\"\"\n",
    "    return get_injury_base_dir().joinpath(*parts).resolve()\n",
    "\n",
    "# ── One‑shot: ensure all declared dirs exist at import‑time ───────────────────\n",
    "for _p in (\n",
    "    DATA_DIR, RAW_DIR, DEBUG_DIR, EXPORTS_DIR, INJURY_DIR,\n",
    "    SPOTRAC_DIR, SILVER_DIR, FINAL_DIR, SPOTRAC_DEBUG_DIR, SPOTRAC_RAW_DIR,\n",
    "    NBA_BASIC_ADVANCED_STATS_DIR, NBA_BASE_DATA_DIR, ADVANCED_METRICS_DIR,\n",
    "    DEFENSE_DATA_DIR, FINAL_DATASET_DIR,\n",
    "    PLAY_TYPES_DIR   # ← ensure synergy play‑types folder exists\n",
    "):\n",
    "    _p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "print(\"all directories:\")\n",
    "print(\"root directory:\")\n",
    "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"RAW_DIR: {RAW_DIR}\")\n",
    "print(f\"DEBUG_DIR: {DEBUG_DIR}\")\n",
    "print(f\"EXPORTS_DIR: {EXPORTS_DIR}\")\n",
    "print(f\"INJURY_DIR: {INJURY_DIR}\")\n",
    "print(f\"INJURY_DATASETS_DIR: {INJURY_DATASETS_DIR}\")\n",
    "print(f\"NBA_BASIC_ADVANCED_STATS_DIR: {NBA_BASIC_ADVANCED_STATS_DIR}\")\n",
    "print(f\"NBA_BASE_DATA_DIR: {NBA_BASE_DATA_DIR}\")\n",
    "print(f\"ADVANCED_METRICS_DIR: {ADVANCED_METRICS_DIR}\")\n",
    "print(f\"DEFENSE_DATA_DIR: {DEFENSE_DATA_DIR}\")\n",
    "print(f\"FINAL_DATASET_DIR: {FINAL_DATASET_DIR}\")\n",
    "print(f\"PLAY_TYPES_DIR: {PLAY_TYPES_DIR}\")\n",
    "print(f\"SPOTRAC_DIR: {SPOTRAC_DIR}\")\n",
    "print(f\"SILVER_DIR: {SILVER_DIR}\")\n",
    "print(f\"FINAL_DIR: {FINAL_DIR}\")\n",
    "print(f\"SPOTRAC_DEBUG_DIR: {SPOTRAC_DEBUG_DIR}\")\n",
    "print(f\"SPOTRAC_RAW_DIR: {SPOTRAC_RAW_DIR}\")\n",
    "print(\"all directories:\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc3148b",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d80967a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/utils/airflow_helpers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/utils/airflow_helpers.py\n",
    "\"\"\"\n",
    "Shared utilities for Airflow DAGs to eliminate common AI-generated patterns.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Union\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "def format_season_range(start_year: int, end_year: int) -> List[str]:\n",
    "    \"\"\"Generate season strings like '2015-16', '2016-17' from year range.\"\"\"\n",
    "    return [f\"{y}-{(y+1)%100:02d}\" for y in range(start_year, end_year + 1)]\n",
    "\n",
    "\n",
    "\n",
    "def normalize_xcom_input(xcom_value) -> Union[str, List[str], None]:\n",
    "    \"\"\"Handle Airflow's LazyXComSequence and other XCom types consistently.\"\"\"\n",
    "    if xcom_value is None:\n",
    "        return None\n",
    "    \n",
    "    # Handle LazyXComSequence (Airflow 3+)\n",
    "    try:\n",
    "        from airflow.sdk.execution_time.lazy_sequence import LazyXComSequence\n",
    "        if isinstance(xcom_value, LazyXComSequence):\n",
    "            xcom_value = list(xcom_value)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # Handle other iterables\n",
    "    if hasattr(xcom_value, \"__iter__\") and not isinstance(xcom_value, (str, Path)):\n",
    "        xcom_value = list(xcom_value)\n",
    "    \n",
    "    # If it's a list, return the first element (common pattern)\n",
    "    if isinstance(xcom_value, list):\n",
    "        return xcom_value[0] if xcom_value else None\n",
    "    \n",
    "    return xcom_value\n",
    "\n",
    "\n",
    "def load_cached_seasons(parquet_path: Path, force_full: bool) -> List[str]:\n",
    "    \"\"\"Load existing seasons from cache, respecting force_full flag.\"\"\"\n",
    "    if parquet_path.exists() and not force_full:\n",
    "        return pd.read_parquet(parquet_path)[\"season\"].unique().tolist()\n",
    "    return []\n",
    "\n",
    "\n",
    "def merge_parquet_shards(shard_paths: List[str], final_path: Path, \n",
    "                        force_full: bool = False, \n",
    "                        dedup_cols: Optional[List[str]] = None) -> str:\n",
    "    \"\"\"Merge parquet shards with caching logic.\"\"\"\n",
    "    clean_paths = [p for p in shard_paths if p]\n",
    "    \n",
    "    if not clean_paths:\n",
    "        if final_path.exists():\n",
    "            print(f\"No new shards; using existing {final_path}\")\n",
    "            return str(final_path)\n",
    "        raise RuntimeError(\"No shards provided and no cache present\")\n",
    "    \n",
    "    fresh_dfs = [pd.read_parquet(p) for p in clean_paths]\n",
    "    merged_fresh = pd.concat(fresh_dfs, ignore_index=True)\n",
    "    \n",
    "    if final_path.exists() and not force_full:\n",
    "        cached = pd.read_parquet(final_path)\n",
    "        merged = pd.concat([cached, merged_fresh], ignore_index=True)\n",
    "        if dedup_cols:\n",
    "            merged = merged.drop_duplicates(subset=dedup_cols)\n",
    "    else:\n",
    "        merged = merged_fresh\n",
    "    \n",
    "    final_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    merged.to_parquet(final_path, index=False)\n",
    "    return str(final_path)\n",
    "\n",
    "\n",
    "def validate_required_columns(df: pd.DataFrame, required_cols: set, name: str = \"dataframe\"):\n",
    "    \"\"\"Validate that required columns exist in dataframe.\"\"\"\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"{name} missing required columns: {missing}\")\n",
    "\n",
    "\n",
    "def normalize_header(header: str) -> str:\n",
    "    \"\"\"Normalize header for robust matching.\"\"\"\n",
    "    return re.sub(r'[^a-z]', '', header.lower())\n",
    "\n",
    "\n",
    "def extract_year_from_path(path: Union[str, Path]) -> Optional[int]:\n",
    "    \"\"\"Extract year from filename like 'advanced_2024-25.parquet' -> 2024.\"\"\"\n",
    "    match = re.search(r'(\\d{4})', Path(path).stem)\n",
    "    return int(match.group(1)) if match else None "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e241fb4",
   "metadata": {},
   "source": [
    "# Dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "951d4d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/nba_player_estimated_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/nba_player_estimated_metrics.py\n",
    "\"\"\"\n",
    "Pull NBA Player Estimated Metrics from stats.nba.com via nba_api.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import pendulum\n",
    "import pandas as pd\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import get_current_context\n",
    "\n",
    "from eda.nba_em_connector import (\n",
    "    fetch_player_estimated_metrics,\n",
    "    normalize_player_estimated_metrics,\n",
    ")\n",
    "from utils.storage import write_final_dataset\n",
    "from utils.airflow_helpers import format_season_range, normalize_xcom_input, load_cached_seasons\n",
    "\n",
    "# Config\n",
    "SEASON_START_DEFAULT = \"2015-16\"\n",
    "SEASON_END_DEFAULT = \"2024-25\"\n",
    "SEASON_TYPE_DEFAULT = \"Regular Season\"\n",
    "\n",
    "BASE_DIR = Path(\"api/src/airflow_project/data\")\n",
    "RAW_DIR = BASE_DIR / \"raw\" / \"player_est_metrics\"\n",
    "SILVER_DIR = BASE_DIR / \"silver\" / \"player_est_metrics\"\n",
    "FINAL_DIR = BASE_DIR / \"final\"\n",
    "\n",
    "FINAL_PARQUET = FINAL_DIR / \"player_est_metrics_all.parquet\"\n",
    "\n",
    "REQ_COLS = {\n",
    "    \"PLAYER_ID\", \"PLAYER_NAME\", \"E_OFF_RATING\", \"E_DEF_RATING\", \n",
    "    \"E_NET_RATING\", \"E_USG_PCT\", \"E_PACE\", \"E_AST_RATIO\", \n",
    "    \"SEASON\", \"SEASON_TYPE\",\n",
    "}\n",
    "\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"nba_player_estimated_metrics\",\n",
    "    start_date=pendulum.datetime(2025, 7, 1, tz=\"America/New_York\"),\n",
    "    schedule=\"15 4 * * *\",\n",
    "    catchup=False,\n",
    "    tags=[\"nba\", \"player_estimated_metrics\", \"nba_api\"],\n",
    "    default_args={\"retries\": 2},\n",
    "    params={\n",
    "        \"seasons\": None,\n",
    "        \"season_start\": SEASON_START_DEFAULT,\n",
    "        \"season_end\": SEASON_END_DEFAULT,\n",
    "        \"season_type\": SEASON_TYPE_DEFAULT,\n",
    "        \"force_full\": False,\n",
    "    },\n",
    ")\n",
    "def nba_player_estimated_metrics_dag():\n",
    "\n",
    "    @task\n",
    "    def determine_seasons() -> Dict[str, List[str] | bool | str]:\n",
    "        \"\"\"Figure out which seasons to process.\"\"\"\n",
    "        ctx = get_current_context()\n",
    "        p = ctx[\"params\"]\n",
    "\n",
    "        if p.get(\"seasons\"):\n",
    "            requested = list(map(str, p[\"seasons\"]))\n",
    "        else:\n",
    "            start_year = int(p.get(\"season_start\", SEASON_START_DEFAULT).split(\"-\")[0])\n",
    "            end_year = int(p.get(\"season_end\", SEASON_END_DEFAULT).split(\"-\")[0])\n",
    "            requested = format_season_range(start_year, end_year)\n",
    "\n",
    "        season_type = str(p.get(\"season_type\", SEASON_TYPE_DEFAULT))\n",
    "        force_full = bool(p.get(\"force_full\", False))\n",
    "\n",
    "        existing = load_cached_seasons(FINAL_PARQUET, force_full)\n",
    "        missing = sorted(set(requested) - set(existing))\n",
    "\n",
    "        print(f\"[determine_seasons] requested={requested}, existing={existing}, missing={missing}, force_full={force_full}\")\n",
    "\n",
    "        return {\n",
    "            \"requested\": requested,\n",
    "            \"missing\": missing,\n",
    "            \"season_type\": season_type,\n",
    "            \"force_full\": force_full,\n",
    "        }\n",
    "\n",
    "    @task\n",
    "    def build_fetch_payloads(season_info: Dict[str, List[str] | bool | str]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Build payloads for expand_kwargs.\"\"\"\n",
    "        season_type = str(season_info[\"season_type\"])\n",
    "        payloads = [{\"season\": s, \"season_type\": season_type} for s in season_info[\"missing\"]]\n",
    "        print(f\"[build_fetch_payloads] {len(payloads)} payloads\")\n",
    "        return payloads\n",
    "\n",
    "    @task\n",
    "    def fetch_one_season(season: str, season_type: str) -> Dict[str, str]:\n",
    "        \"\"\"Fetch raw data for a single season.\"\"\"\n",
    "        RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        result = fetch_player_estimated_metrics(\n",
    "            season=season,\n",
    "            season_type=season_type,\n",
    "            raw_dir=RAW_DIR\n",
    "        )\n",
    "        print(f\"[fetch_one_season] {season} saved {list(result.keys())}\")\n",
    "        return {k: str(v) for k, v in result.items()}\n",
    "\n",
    "    @task\n",
    "    def collect_fetched(fetched):\n",
    "        \"\"\"Materialize LazyXComSequence to list.\"\"\"\n",
    "        return list(fetched)\n",
    "\n",
    "    @task(multiple_outputs=True)\n",
    "    def normalize_shards(saved_paths_dicts):\n",
    "        \"\"\"Normalize shards and combine into temp parquet.\"\"\"\n",
    "        saved_paths_dicts = normalize_xcom_input(saved_paths_dicts)\n",
    "        if not isinstance(saved_paths_dicts, list):\n",
    "            saved_paths_dicts = list(saved_paths_dicts)\n",
    "\n",
    "        SILVER_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        final_paths_all = []\n",
    "        df_frames = []\n",
    "\n",
    "        for paths in saved_paths_dicts:\n",
    "            norm_paths, df = normalize_player_estimated_metrics(\n",
    "                {k: Path(v) for k, v in paths.items()},\n",
    "                silver_dir=SILVER_DIR\n",
    "            )\n",
    "            final_paths_all.extend([str(p) for p in norm_paths])\n",
    "            if not df.empty:\n",
    "                df_frames.append(df)\n",
    "\n",
    "        tmp = pd.concat(df_frames, ignore_index=True) if df_frames else pd.DataFrame()\n",
    "        tmp_path = SILVER_DIR / \"_tmp_combined_current_run.parquet\"\n",
    "        tmp.to_parquet(tmp_path, index=False)\n",
    "\n",
    "        print(f\"[normalize_shards] combined {len(tmp)} rows\")\n",
    "        return {\"final_paths\": final_paths_all, \"combined_tmp\": str(tmp_path)}\n",
    "\n",
    "    @task\n",
    "    def merge_to_final(tmp_path: str, season_info: Dict[str, List[str] | bool | str]) -> str:\n",
    "        \"\"\"Merge temp parquet into final dataset.\"\"\"\n",
    "        tmp_path = Path(tmp_path) if tmp_path else None\n",
    "\n",
    "        if tmp_path and tmp_path.exists():\n",
    "            fresh = pd.read_parquet(tmp_path)\n",
    "        else:\n",
    "            fresh = pd.DataFrame()\n",
    "\n",
    "        if fresh.empty and FINAL_PARQUET.exists() and not season_info[\"force_full\"]:\n",
    "            print(\"[merge_to_final] No new data; using cached final.\")\n",
    "            return str(FINAL_PARQUET)\n",
    "\n",
    "        if FINAL_PARQUET.exists() and not season_info[\"force_full\"]:\n",
    "            cached = pd.read_parquet(FINAL_PARQUET)\n",
    "            merged = pd.concat([cached, fresh], ignore_index=True)\n",
    "        else:\n",
    "            merged = fresh\n",
    "\n",
    "        if not merged.empty:\n",
    "            missing_cols = {\"SEASON\", \"SEASON_TYPE\"} - set(merged.columns)\n",
    "            if missing_cols:\n",
    "                raise ValueError(f\"Missing required columns for deduplication: {missing_cols}\")\n",
    "            merged = merged.drop_duplicates(subset=[\"PLAYER_ID\", \"SEASON\", \"SEASON_TYPE\"])\n",
    "\n",
    "        FINAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        final_path = write_final_dataset(merged, FINAL_PARQUET)\n",
    "        print(f\"[merge_to_final] final {len(merged)} rows\")\n",
    "        return str(final_path)\n",
    "\n",
    "    @task\n",
    "    def validate_outputs(final_pq: str) -> None:\n",
    "        \"\"\"Validate final dataset.\"\"\"\n",
    "        df = pd.read_parquet(final_pq)\n",
    "\n",
    "        missing = REQ_COLS - set(df.columns)\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "        print(\"Rows by SEASON:\")\n",
    "        print(df.groupby(\"SEASON\").size())\n",
    "        print(\"Rows by SEASON_TYPE:\")\n",
    "        print(df.groupby(\"SEASON_TYPE\").size())\n",
    "        print(\"Top 5 E_NET_RATING by season:\")\n",
    "        print(df.sort_values(\"E_NET_RATING\", ascending=False)\n",
    "              .groupby(\"SEASON\")\n",
    "              .head(5)[[\"PLAYER_NAME\", \"E_NET_RATING\", \"SEASON\"]]\n",
    "              .to_string(index=False))\n",
    "\n",
    "    # Workflow\n",
    "    season_info = determine_seasons()\n",
    "    payloads = build_fetch_payloads(season_info)\n",
    "    fetched = fetch_one_season.expand_kwargs(payloads)\n",
    "    collected = collect_fetched(fetched)\n",
    "    norm_res = normalize_shards(collected)\n",
    "    final_pq = merge_to_final(tmp_path=norm_res[\"combined_tmp\"], season_info=season_info)\n",
    "    validate_outputs(final_pq)\n",
    "\n",
    "nba_player_estimated_metrics_dag()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389fde3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/new_injury_reports.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/new_injury_reports.py\n",
    "\"\"\"\n",
    "## NBA Injury Reports ETL (ESPN + others)\n",
    "\n",
    "Daily pipeline that:\n",
    "1. Determines which dates to ingest (default: yesterday -> today).\n",
    "2. Fetches and stores injury reports (one task per day via dynamic mapping).\n",
    "3. Runs a simple validation.\n",
    "\n",
    "Implements TaskFlow API & dynamic task mapping. :contentReference[oaicite:1]{index=1}\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from airflow.sdk.execution_time.lazy_sequence import LazyXComSequence  # NEW\n",
    "import datetime as dt\n",
    "import pendulum\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import get_current_context\n",
    "\n",
    "# Import your module\n",
    "from eda.new_injury_reports import run_daily_ingestion, clean_and_normalize, incremental_store, make_sources\n",
    "from eda.combining_injury_reports import (\n",
    "    build_full_dataset,  # now the new function\n",
    "    injury_path,\n",
    ")\n",
    "from utils.config import injury_path\n",
    "\n",
    "# -------- Config defaults (override with DAG Params) -----------------\n",
    "# DEFAULT_OUTPUT = Path(\"api/src/airflow_project/data/injury_reports\")\n",
    "# Use the canonical injury_reports directory from config:\n",
    "DEFAULT_OUTPUT = injury_path(\"\")      # returns data/injury_reports\n",
    "print(f\"DEFAULT_OUTPUT: {DEFAULT_OUTPUT}\")\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"injury_reports_daily\",\n",
    "    start_date=pendulum.datetime(2025, 7, 1, tz=\"America/New_York\"),\n",
    "    schedule=\"0 5 * * *\",             # 05:00 ET daily pull\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    tags=[\"nba\", \"injuries\", \"espn\"],\n",
    "    default_args={\"retries\": 1},\n",
    "    params={\n",
    "        \"start_date\": None,           # ISO yyyy-mm-dd\n",
    "        \"end_date\": None,             # ISO yyyy-mm-dd\n",
    "        \"days_back\": 1,               # if no start/end given\n",
    "        \"force_full\": False,          # not used yet, kept for parity\n",
    "        \"fix_return_year\": True,\n",
    "        \"season_start_year\": 2024,    # NEW: season configuration\n",
    "        \"allowed_years\": [2024, 2025], # computed from season_start_year\n",
    "        \"statsurge_root\": \"data/statsurge\",\n",
    "        \"parquet_path\": str(DEFAULT_OUTPUT / \"injuries_primary.parquet\"),\n",
    "    },\n",
    ")\n",
    "def injury_reports_daily():\n",
    "\n",
    "    @task\n",
    "    def determine_dates() -> list[str]:\n",
    "        \"\"\"\n",
    "        Decide which dates to fetch.\n",
    "        Priority:\n",
    "          1. explicit start_date/end_date params\n",
    "          2. days_back param (N days ending yesterday)\n",
    "        Returns list of ISO date strings.\n",
    "        \"\"\"\n",
    "        ctx = get_current_context()\n",
    "        p = ctx[\"params\"]\n",
    "\n",
    "        if p.get(\"start_date\") and p.get(\"end_date\"):\n",
    "            start = dt.date.fromisoformat(p[\"start_date\"])\n",
    "            end = dt.date.fromisoformat(p[\"end_date\"])\n",
    "        else:\n",
    "            days_back = int(p.get(\"days_back\", 1))\n",
    "            end = dt.date.today()\n",
    "            start = end - dt.timedelta(days=days_back)\n",
    "\n",
    "        dates = [ (start + dt.timedelta(days=i)).isoformat()\n",
    "                  for i in range((end - start).days + 1) ]\n",
    "\n",
    "        print(f\"[determine_dates] start={start} end={end} -> {len(dates)} days\")\n",
    "        return dates\n",
    "\n",
    "    @task\n",
    "    def fetch_one_day(date_str: str) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Ingest one day's injuries and upsert to parquet.\n",
    "        RETURNS a dict so downstream can safely access by string key:\n",
    "          {\"parquet_path\": \"<path/to/file.parquet>\"}\n",
    "        \"\"\"\n",
    "        ctx = get_current_context()\n",
    "        p = ctx[\"params\"]\n",
    "\n",
    "        date = dt.date.fromisoformat(date_str)\n",
    "        # parquet_path = Path(p[\"parquet_path\"])\n",
    "        parquet_path = injury_path(\"injuries_primary.parquet\")\n",
    "\n",
    "        # Season wiring ----------------------------------------------------------\n",
    "        season_start = int(p.get(\"season_start_year\", 2024))\n",
    "        allowed_years = (season_start, season_start + 1)\n",
    "\n",
    "        # Let run_daily_ingestion build sources internally to avoid duplication\n",
    "        df_day = run_daily_ingestion(\n",
    "            date=date,\n",
    "            sources=None,  # Let it build sources internally\n",
    "            cleaner=clean_and_normalize,\n",
    "            store_fn=lambda d: incremental_store(d, path=parquet_path),\n",
    "            drop_duplicates_on=[\"player_name\",\"team\",\"status\",\"body_part\",\"report_date\",\"est_return_date\"],\n",
    "            fix_return_year=bool(p.get(\"fix_return_year\", True)),\n",
    "            allowed_years=allowed_years,\n",
    "            statsurge_root=p.get(\"statsurge_root\", \"data/statsurge\"),\n",
    "        )\n",
    "        print(f\"[fetch_one_day] {date} rows={len(df_day)} -> {parquet_path}\")\n",
    "\n",
    "        return {\"parquet_path\": str(parquet_path)}\n",
    "\n",
    "    @task\n",
    "    def final_check(parquet_path: str | list[str] | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Basic validation. Handles Airflow LazyXComSequence / list / str safely.\n",
    "        \"\"\"\n",
    "        from airflow.sdk.execution_time.lazy_sequence import LazyXComSequence  # optional import guard\n",
    "\n",
    "        print(f\"[final_check] Received parquet_path type: {type(parquet_path)}\")\n",
    "        print(f\"[final_check] Received parquet_path value: {parquet_path}\")\n",
    "\n",
    "        if parquet_path is None:\n",
    "            raise ValueError(\"final_check received no parquet_path\")\n",
    "\n",
    "        # Normalize lazy proxies or other iterables to a list\n",
    "        if isinstance(parquet_path, LazyXComSequence) or (\n",
    "            not isinstance(parquet_path, (str, Path)) and hasattr(parquet_path, \"__iter__\")\n",
    "        ):\n",
    "            parquet_path = list(parquet_path)\n",
    "\n",
    "        # If we ended up with a list, pick the first element (or iterate/validate all)\n",
    "        if isinstance(parquet_path, list):\n",
    "            if not parquet_path:\n",
    "                raise ValueError(\"final_check received an empty list for parquet_path\")\n",
    "            parquet_path = parquet_path[0]\n",
    "\n",
    "        pq = Path(parquet_path)\n",
    "        if not pq.exists():\n",
    "            raise FileNotFoundError(f\"Parquet file not found: {pq}\")\n",
    "\n",
    "        df = pd.read_parquet(pq)\n",
    "        req = {\"player_name\",\"team\",\"status\",\"report_date\",\"source\"}\n",
    "        miss = req - set(df.columns)\n",
    "        assert not miss, f\"Missing columns: {miss}\"\n",
    "\n",
    "        print(f\"[final_check] Successfully validated {len(df)} rows\")\n",
    "        print(\"Rows by report_date:\\n\", df.groupby(\"report_date\").size())\n",
    "\n",
    "    @task\n",
    "    def combine_full_dataset(parquet_path: str | list[str] | LazyXComSequence | None = None) -> str:\n",
    "        # 1. Normalize lazy proxies or other iterables to a list\n",
    "        if isinstance(parquet_path, LazyXComSequence) \\\n",
    "        or (not isinstance(parquet_path, (str, Path)) and hasattr(parquet_path, \"__iter__\")):\n",
    "            parquet_path = list(parquet_path)\n",
    "\n",
    "        # 2. If it’s a list, pick the first element\n",
    "        if isinstance(parquet_path, list):\n",
    "            if not parquet_path:\n",
    "                raise ValueError(\"combine_full_dataset received an empty list for parquet_path\")\n",
    "            parquet_path = parquet_path[0]\n",
    "\n",
    "        # 3. Now it’s safe to build a Path\n",
    "        file_a = Path(parquet_path).resolve()\n",
    "        file_b = injury_path(\"NBA Player Injury Stats(1951 - 2023).parquet\")\n",
    "        out_dir = injury_path(\"\")\n",
    "\n",
    "        full_path = build_full_dataset(\n",
    "            file_a=file_a,\n",
    "            file_b=file_b,\n",
    "            out_dir=out_dir,\n",
    "            full_out_name=\"historical_injuries_1951_2025_clean.parquet\",\n",
    "            duplicate_strategy=\"keep_first\",\n",
    "            write_intermediates=False,\n",
    "        )\n",
    "        return str(full_path)\n",
    "\n",
    "\n",
    "    # ---- DAG flow ----\n",
    "    date_list = determine_dates()\n",
    "    paths = fetch_one_day.expand(date_str=date_list)\n",
    "\n",
    "    # SAFE string-key lookup; no int indexing\n",
    "    # Airflow will automatically reduce multiple identical paths to a single value\n",
    "    final_check(parquet_path=paths[\"parquet_path\"])\n",
    "    combined = combine_full_dataset(parquet_path=paths[\"parquet_path\"])\n",
    "\n",
    "injury_reports_daily()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53ac1882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/spotrac_misc_assets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/spotrac_misc_assets.py\n",
    "\n",
    "\"\"\"\n",
    "Fetch Spotrac misc assets and save to final directory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import pendulum, pandas as pd\n",
    "import re\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import get_current_context\n",
    "from airflow.exceptions import AirflowSkipException\n",
    "import os\n",
    "\n",
    "from eda.spotrac_connector import (\n",
    "    fetch_spotrac_salary_cap_history,\n",
    "    fetch_spotrac_multi_year_tracker,\n",
    "    fetch_spotrac_cash_tracker,\n",
    "    fetch_spotrac_extensions,\n",
    "    fetch_spotrac_tax_tracker,\n",
    "    fetch_spotrac_transactions,\n",
    ")\n",
    "from utils.storage import write_final_dataset\n",
    "from utils.config import RAW_DIR, DEBUG_DIR, FINAL_DIR, SPOTRAC_RAW_DIR, SPOTRAC_DEBUG_DIR\n",
    "from utils.airflow_helpers import normalize_header, extract_year_from_path\n",
    "\n",
    "BASE_DIR = RAW_DIR.parent\n",
    "\n",
    "\n",
    "def resolve_column_aliases(df, column_map: dict[str, list[str]], \n",
    "                          extra_fixers: dict[str, callable] = None):\n",
    "    \"\"\"Rename columns to standard names using aliases.\"\"\"\n",
    "    rename_log = {}\n",
    "    cols_raw = list(df.columns)\n",
    "    cols_norm = {normalize_header(c): c for c in cols_raw}\n",
    "\n",
    "    for target, aliases in column_map.items():\n",
    "        if target in df.columns:\n",
    "            continue\n",
    "        found = None\n",
    "        for alias in aliases:\n",
    "            norm = normalize_header(alias)\n",
    "            if norm in cols_norm:\n",
    "                found = cols_norm[norm]\n",
    "                break\n",
    "        if found:\n",
    "            df = df.rename(columns={found: target})\n",
    "            rename_log[found] = target\n",
    "        elif extra_fixers and target in extra_fixers:\n",
    "            df = extra_fixers[target](df)\n",
    "\n",
    "    missing = [k for k in column_map if k not in df.columns]\n",
    "    return df, missing, rename_log\n",
    "\n",
    "\n",
    "@dag(\n",
    "    dag_id=\"spotrac_misc_assets\",\n",
    "    start_date=pendulum.datetime(2025, 7, 1, tz=\"America/New_York\"),\n",
    "    schedule=\"15 4 * * *\",\n",
    "    catchup=False,\n",
    "    tags=[\"nba\", \"spotrac\", \"misc\"],\n",
    "    default_args={\"retries\": 2},\n",
    "    params={\n",
    "        \"season_start\": 2015,\n",
    "        \"season_end\": 2025,\n",
    "        \"recent_years\": 3,\n",
    "        \"txn_days\": 7,\n",
    "        \"force_full\": False,\n",
    "    },\n",
    ")\n",
    "def spotrac_misc_assets():\n",
    "\n",
    "    @task\n",
    "    def determine_years() -> dict:\n",
    "        \"\"\"Build year lists and flags.\"\"\"\n",
    "        ctx = get_current_context()[\"params\"]\n",
    "        yr_all = list(range(int(ctx[\"season_start\"]), int(ctx[\"season_end\"]) + 1))\n",
    "        recent_n = int(ctx[\"recent_years\"])\n",
    "        info = {\n",
    "            \"all_years\": yr_all,\n",
    "            \"recent_years\": yr_all[-recent_n:],\n",
    "            \"txn_days\": int(ctx[\"txn_days\"]),\n",
    "            \"force_full\": bool(ctx[\"force_full\"]),\n",
    "        }\n",
    "        print(f\"[determine_years] {info}\")\n",
    "        return info\n",
    "\n",
    "    @task\n",
    "    def get_recent_years(info: dict) -> list[int]:\n",
    "        \"\"\"Extract recent years for mapping.\"\"\"\n",
    "        yrs = info[\"recent_years\"]\n",
    "        print(f\"[get_recent_years] {yrs}\")\n",
    "        return yrs\n",
    "\n",
    "    # Static assets\n",
    "    @task\n",
    "    def fetch_cba() -> str:\n",
    "        \"\"\"Fetch CBA history.\"\"\"\n",
    "        out_path, df, _ = fetch_spotrac_salary_cap_history(\n",
    "            raw_dir=SPOTRAC_RAW_DIR,\n",
    "            debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "        )\n",
    "        abs_out = str(Path(out_path).resolve())\n",
    "        print(f\"[fetch_cba] rows={len(df)} cols={df.shape[1]} -> {abs_out}\")\n",
    "        return abs_out\n",
    "\n",
    "    @task\n",
    "    def fetch_multi_cap() -> str:\n",
    "        out, df, _ = fetch_spotrac_multi_year_tracker(\n",
    "            raw_dir=SPOTRAC_RAW_DIR,\n",
    "            debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "        )\n",
    "        print(f\"[fetch_multi_cap] rows={len(df)} cols={df.shape[1]} -> {out}\")\n",
    "        return str(out)\n",
    "\n",
    "    @task\n",
    "    def fetch_multi_cash() -> str:\n",
    "        out, df, _ = fetch_spotrac_cash_tracker(\n",
    "            raw_dir=SPOTRAC_RAW_DIR,\n",
    "            debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "        )\n",
    "        print(f\"[fetch_multi_cash] rows={len(df)} cols={df.shape[1]} -> {out}\")\n",
    "        return str(out)\n",
    "\n",
    "    # Yearly extensions/tax\n",
    "    @task\n",
    "    def fetch_extension(year: int) -> str | None:\n",
    "        \"\"\"Fetch extensions for a single year.\"\"\"\n",
    "        try:\n",
    "            out, df, _ = fetch_spotrac_extensions(\n",
    "                year,\n",
    "                raw_dir=SPOTRAC_RAW_DIR,\n",
    "                debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "            )\n",
    "            print(f\"[fetch_extension] year={year} rows={len(df)} -> {out}\")\n",
    "            return str(out)\n",
    "        except RuntimeError as e:\n",
    "            print(f\"[fetch_extension] year={year} failed: {e}\")\n",
    "            raise AirflowSkipException(f\"No extensions table for {year}\")\n",
    "\n",
    "    @task\n",
    "    def fetch_tax(year: int) -> str:\n",
    "        out, df, _ = fetch_spotrac_tax_tracker(\n",
    "            year,\n",
    "            raw_dir=SPOTRAC_RAW_DIR,\n",
    "            debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "        )\n",
    "        print(f\"[fetch_tax] year={year} rows={len(df)} -> {out}\")\n",
    "        return str(out)\n",
    "\n",
    "    # Rolling transactions\n",
    "    @task\n",
    "    def fetch_txn(info: dict) -> str:\n",
    "        \"\"\"Pull transactions for rolling window.\"\"\"\n",
    "        from datetime import date, timedelta\n",
    "        end = date.today()\n",
    "        start = end - timedelta(days=info[\"txn_days\"])\n",
    "        out, df, _ = fetch_spotrac_transactions(\n",
    "            start.isoformat(),\n",
    "            end.isoformat(),\n",
    "            raw_dir=SPOTRAC_RAW_DIR,\n",
    "            debug_dir=SPOTRAC_DEBUG_DIR,\n",
    "        )\n",
    "        if out:\n",
    "            print(f\"[fetch_txn] {start}->{end} rows={len(df)} -> {out}\")\n",
    "            return str(out)\n",
    "        print(f\"[fetch_txn] {start}->{end} no data\")\n",
    "        return \"\"\n",
    "\n",
    "    # Merge helpers\n",
    "    @task\n",
    "    def merge_yearly(paths: list[str | None], name: str) -> str:\n",
    "        \"\"\"Merge parquet shards from mapped tasks.\"\"\"\n",
    "        clean = [p for p in paths if p]\n",
    "        final = FINAL_DIR / f\"{name}_all.parquet\"\n",
    "        print(f\"[merge_yearly] merging {len(clean)} shards of {name}\")\n",
    "\n",
    "        if clean:\n",
    "            dfs = [pd.read_parquet(p) for p in clean]\n",
    "            merged = pd.concat(dfs, ignore_index=True)\n",
    "            write_final_dataset(merged, final)\n",
    "            print(f\"[merge_yearly] wrote {final}\")\n",
    "            return str(final)\n",
    "\n",
    "        if final.exists():\n",
    "            print(f\"[merge_yearly] using existing {final}\")\n",
    "            return str(final)\n",
    "\n",
    "        print(f\"[merge_yearly] no data available\")\n",
    "        return \"\"\n",
    "\n",
    "    @task\n",
    "    def validate_assets(cba: str, cap: str, cash: str,\n",
    "                        ext: str, tax: str, txn: str) -> None:\n",
    "        \"\"\"Validate all produced parquet assets.\"\"\"\n",
    "        import json\n",
    "\n",
    "        input_paths = {\n",
    "            \"cba\": cba,\n",
    "            \"multi_cap\": cap,\n",
    "            \"multi_cash\": cash,\n",
    "            \"extensions\": ext,\n",
    "            \"tax\": tax,\n",
    "            \"txn\": txn\n",
    "        }\n",
    "        mandatory = {\"cba\", \"multi_cap\", \"multi_cash\"}\n",
    "\n",
    "        # Check existence\n",
    "        valid_paths = {}\n",
    "        for label, p in input_paths.items():\n",
    "            if not p or not os.path.isfile(p):\n",
    "                if label in mandatory:\n",
    "                    raise FileNotFoundError(f\"{label} file missing: {p}\")\n",
    "                print(f\"[validate_assets] {label} skipped\")\n",
    "            else:\n",
    "                valid_paths[label] = p\n",
    "\n",
    "        print(f\"[validate_assets] validating: {list(valid_paths.keys())}\")\n",
    "\n",
    "        # Column mapping\n",
    "        column_maps = {\n",
    "            \"cba\": {\"year\": [\"season\", \"yr\"]},\n",
    "            \"multi_cap\": {\"team\": [\"team\", \"teamname\"]},\n",
    "            \"multi_cash\": {\"team\": [\"team\", \"teamname\"]},\n",
    "            \"extensions\": {\n",
    "                \"player\": [\"players\"],\n",
    "                \"total\": [\"total value\", \"value\", \"amount\", \"totalvalue\"],\n",
    "                \"year\": [\"yr\", \"season\"],\n",
    "            },\n",
    "            \"tax\": {\n",
    "                \"team\": [\"team\", \"teamname\"],\n",
    "                \"tax\": [\"tax bill\", \"taxbill\", \"luxurytax\", \"taxthreshold\"],\n",
    "                \"year\": [\"yr\", \"season\"],\n",
    "            },\n",
    "            \"txn\": {\n",
    "                \"date\": [\"transactiondate\"],\n",
    "                \"player\": [\"players\", \"player(s)\"],\n",
    "                \"type\": [\"details\", \"move\", \"transactiontype\"],\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # Setup caching\n",
    "        cache_dir = DEBUG_DIR / \"validate\"\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        schema_cache = cache_dir / \"validate_schema_cache.json\"\n",
    "        stats_cache = cache_dir / \"validate_stats_cache.json\"\n",
    "\n",
    "        old_schema = {}\n",
    "        old_stats = {}\n",
    "        if schema_cache.exists():\n",
    "            old_schema = pd.read_json(schema_cache, typ=\"series\").to_dict()\n",
    "        if stats_cache.exists():\n",
    "            old_stats = json.loads(stats_cache.read_text())\n",
    "\n",
    "        drift_report = {}\n",
    "        row_drift = {}\n",
    "        current_stats = {}\n",
    "\n",
    "        for label, p in valid_paths.items():\n",
    "            df = pd.read_parquet(p)\n",
    "\n",
    "            # Schema drift check\n",
    "            prev_cols = set(old_schema.get(label, []))\n",
    "            curr_cols = set(df.columns.tolist())\n",
    "            added = sorted(curr_cols - prev_cols)\n",
    "            removed = sorted(prev_cols - curr_cols)\n",
    "            if added or removed:\n",
    "                drift_report[label] = {\"added\": added, \"removed\": removed}\n",
    "\n",
    "            # Resolve column aliases\n",
    "            logical_req = column_maps[label]\n",
    "\n",
    "            def add_year_if_missing(dfi: pd.DataFrame) -> pd.DataFrame:\n",
    "                y = extract_year_from_path(p)\n",
    "                return dfi.assign(year=y) if y is not None else dfi\n",
    "\n",
    "            dfn, missing, renames = resolve_column_aliases(\n",
    "                df, logical_req, extra_fixers={\"year\": add_year_if_missing}\n",
    "            )\n",
    "            if missing:\n",
    "                raise AssertionError(f\"{label} missing cols: {set(missing)}\")\n",
    "\n",
    "            # Numeric stats\n",
    "            num_cols = [c for c in dfn.columns if pd.api.types.is_numeric_dtype(dfn[c])]\n",
    "            means = {c: float(dfn[c].dropna().mean()) for c in num_cols}\n",
    "            current_stats[label] = {\n",
    "                \"rows\": int(len(dfn)),\n",
    "                \"cols\": int(len(dfn.columns)),\n",
    "                \"means\": means\n",
    "            }\n",
    "\n",
    "            # Compare with previous stats\n",
    "            if label in old_stats:\n",
    "                prev = old_stats[label]\n",
    "                if prev[\"rows\"] != current_stats[label][\"rows\"]:\n",
    "                    row_drift[label] = {\"old\": prev[\"rows\"], \"new\": current_stats[label][\"rows\"]}\n",
    "                \n",
    "                mean_drift = {c: (prev[\"means\"].get(c), means.get(c)) \n",
    "                             for c in set(prev[\"means\"]) | set(means)}\n",
    "                flagged = {}\n",
    "                for c, (old_m, new_m) in mean_drift.items():\n",
    "                    if old_m is None or new_m is None:\n",
    "                        continue\n",
    "                    if old_m == 0 and new_m != 0:\n",
    "                        flagged[c] = (old_m, new_m)\n",
    "                    else:\n",
    "                        rel = abs(new_m - old_m) / (abs(old_m) + 1e-9)\n",
    "                        if rel > 0.05:\n",
    "                            flagged[c] = (old_m, new_m)\n",
    "                if flagged:\n",
    "                    print(f\"[validate_assets] {label} mean drift >5%: {flagged}\")\n",
    "\n",
    "            print(f\"[validate_assets] {label}: rows={len(dfn)} OK; renames={renames}\")\n",
    "\n",
    "        # Persist snapshots\n",
    "        pd.Series({\n",
    "            lbl: pd.read_parquet(p).columns.tolist()\n",
    "            for lbl, p in valid_paths.items()\n",
    "        }).to_json(schema_cache)\n",
    "        stats_cache.write_text(json.dumps(current_stats, indent=2))\n",
    "\n",
    "        if drift_report:\n",
    "            print(\"[validate_assets] SCHEMA DRIFT:\")\n",
    "            for lbl, rep in drift_report.items():\n",
    "                print(f\"  - {lbl}: added={rep['added']} removed={rep['removed']}\")\n",
    "\n",
    "        if row_drift:\n",
    "            print(\"[validate_assets] ROW COUNT DRIFT:\")\n",
    "            for lbl, rep in row_drift.items():\n",
    "                print(f\"  - {lbl}: old={rep['old']} new={rep['new']}\")\n",
    "\n",
    "    # DAG graph\n",
    "    info = determine_years()\n",
    "    recent_list = get_recent_years(info)\n",
    "\n",
    "    cba_p = fetch_cba()\n",
    "    cap_p = fetch_multi_cap()\n",
    "    cash_p = fetch_multi_cash()\n",
    "\n",
    "    ext_paths = fetch_extension.expand(year=recent_list)\n",
    "    tax_paths = fetch_tax.expand(year=recent_list)\n",
    "\n",
    "    ext_final = merge_yearly(ext_paths, name=\"extensions\")\n",
    "    tax_final = merge_yearly(tax_paths, name=\"tax\")\n",
    "\n",
    "    txn_p = fetch_txn(info)\n",
    "\n",
    "    validate_assets(cba_p, cap_p, cash_p, ext_final, tax_final, txn_p)\n",
    "\n",
    "spotrac_misc_assets()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c552ce3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/defensive_metrics_collect.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/defensive_metrics_collect.py\n",
    "\n",
    "\"\"\"\n",
    "Collect NBA defensive‑metrics into parquet using TaskFlow + dynamic mapping.\n",
    "\n",
    "**Params supported**\n",
    "- seasons:        explicit [\"2022‑23\",\"2023‑24\"]  (overrides everything)\n",
    "- season_start:   first season if `seasons` not given\n",
    "- season_end:     last  season if `seasons` not given\n",
    "- force_full:     if True ignore cache & rebuild all\n",
    "\n",
    "Runs at 04:30 ET daily.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Dict, List\n",
    "\n",
    "import pandas as pd\n",
    "import pendulum\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "from airflow.operators.python import get_current_context\n",
    "\n",
    "from eda.defensive_metrics import DefensiveMetricsCollector\n",
    "from utils.storage import write_final_dataset\n",
    "\n",
    "# ---- Config ----\n",
    "BASE_DIR   = Path(\"api/src/airflow_project/data\")\n",
    "RAW_DIR    = BASE_DIR / \"raw\"\n",
    "FINAL_DIR  = BASE_DIR / \"final\"\n",
    "MIN_SEASON = \"2018-19\"        # hard floor in case params go wild\n",
    "MAX_SEASON = \"2024-25\"        # keep in sync with collector\n",
    "\n",
    "# ------------------------------------------------------------------ DAG\n",
    "@dag(\n",
    "    dag_id=\"defensive_metrics_collect\",\n",
    "    start_date=pendulum.datetime(2025, 7, 1, tz=\"America/New_York\"),\n",
    "    schedule=\"30 4 * * *\",\n",
    "    catchup=False,\n",
    "    tags=[\"nba\", \"defense\", \"metrics\"],\n",
    "    default_args={\"retries\": 2},\n",
    "    params={\n",
    "        \"seasons\": None,                # explicit list\n",
    "        \"season_start\": MIN_SEASON,\n",
    "        \"season_end\":   MAX_SEASON,\n",
    "        \"force_full\":   False,\n",
    "    },\n",
    ")\n",
    "def defensive_metrics_collect():\n",
    "    # ---------- helpers ----------\n",
    "    @task\n",
    "    def determine_seasons() -> Dict[str, List[str] | bool]:\n",
    "        \"\"\"\n",
    "        Decide which seasons to fetch by comparing DAG params to cached parquet.\n",
    "        \"\"\"\n",
    "        ctx   = get_current_context()\n",
    "        p     = ctx[\"params\"]\n",
    "\n",
    "        # build requested list\n",
    "        if p.get(\"seasons\"):\n",
    "            requested: List[str] = list(map(str, p[\"seasons\"]))\n",
    "        else:\n",
    "            start = str(p.get(\"season_start\", MIN_SEASON))\n",
    "            end   = str(p.get(\"season_end\",   MAX_SEASON))\n",
    "            # assume season format 'YYYY-YY'\n",
    "            start_year = int(start.split(\"-\")[0])\n",
    "            end_year   = int(end.split(\"-\")[0])\n",
    "            requested  = [f\"{y}-{str(y+1)[-2:]}\" for y in range(start_year, end_year + 1)]\n",
    "\n",
    "        force_full: bool = bool(p.get(\"force_full\", False))\n",
    "\n",
    "        cache = FINAL_DIR / \"def_metrics_all.parquet\"\n",
    "        if cache.exists() and not force_full:\n",
    "            existing = pd.read_parquet(cache)[\"season\"].unique().tolist()\n",
    "        else:\n",
    "            existing = []\n",
    "\n",
    "        missing = sorted(set(requested) - set(existing))\n",
    "\n",
    "        print(\"[determine_seasons] requested:\", requested)\n",
    "        print(\"[determine_seasons] existing :\", existing)\n",
    "        print(\"[determine_seasons] missing  :\", missing)\n",
    "        print(\"[determine_seasons] force_full:\", force_full)\n",
    "\n",
    "        return {\n",
    "            \"requested\": requested,\n",
    "            \"missing\":   missing,\n",
    "            \"force_full\": force_full,\n",
    "        }\n",
    "\n",
    "    # ---------- dynamic fetch ----------\n",
    "    @task\n",
    "    def seasons_for_mapping(meta: dict) -> List[str]:\n",
    "        \"\"\"Return only the seasons we still need (for .expand).\"\"\"\n",
    "        return list(meta[\"missing\"])\n",
    "\n",
    "    @task\n",
    "    def fetch_one_season(season: str) -> str:\n",
    "        \"\"\"\n",
    "        Pull a single season and write a parquet shard into RAW_DIR.\n",
    "        \"\"\"\n",
    "        df = DefensiveMetricsCollector.collect_metrics_for_seasons([season])\n",
    "        out = RAW_DIR / f\"def_metrics_{season}.parquet\"\n",
    "        out.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_parquet(out, index=False)\n",
    "        print(f\"[fetch_one_season] season={season} rows={len(df)} → {out}\")\n",
    "        return str(out)\n",
    "\n",
    "    # ---------- merge / overwrite ----------\n",
    "    @task\n",
    "    def merge_shards(shard_paths: List[str], meta: dict) -> str:\n",
    "        \"\"\"\n",
    "        Merge all shards (new or cached) and overwrite the final parquet.\n",
    "        \"\"\"\n",
    "        final_path = FINAL_DIR / \"def_metrics_all.parquet\"\n",
    "        fresh_dfs  = [pd.read_parquet(p) for p in shard_paths]\n",
    "\n",
    "        if fresh_dfs:\n",
    "            merged_fresh = pd.concat(fresh_dfs, ignore_index=True)\n",
    "            if final_path.exists() and not meta[\"force_full\"]:\n",
    "                cached = pd.read_parquet(final_path)\n",
    "                merged = pd.concat([cached, merged_fresh], ignore_index=True).drop_duplicates(\n",
    "                    subset=[\"PLAYER_NAME\", \"season\"]\n",
    "                )\n",
    "            else:\n",
    "                merged = merged_fresh\n",
    "            final_path = write_final_dataset(merged, final_path)\n",
    "        else:\n",
    "            if final_path.exists():\n",
    "                print(\"[merge_shards] No new seasons; using cached file\", final_path)\n",
    "            else:\n",
    "                raise RuntimeError(\"No shards provided and no cache present.\")\n",
    "\n",
    "        return str(final_path)\n",
    "\n",
    "    # ---------- final assertions ----------\n",
    "    @task\n",
    "    def validate_outputs(final_pq: str) -> None:\n",
    "        df = pd.read_parquet(final_pq)\n",
    "        must_have = {\n",
    "            \"PLAYER_NAME\", \"season\", \"DEF_RATING\", \"dbpm\", \"dws\", \"PLUSMINUS\"\n",
    "        }\n",
    "        missing = must_have - set(df.columns)\n",
    "        assert not missing, f\"Missing cols: {missing}\"\n",
    "        print(\"Rows/season:\\n\", df.groupby(\"season\").size())\n",
    "\n",
    "    # ---------------- DAG orchestration ----------------\n",
    "    meta          = determine_seasons()\n",
    "    seasons_list  = seasons_for_mapping(meta)\n",
    "    shards        = fetch_one_season.expand(season=seasons_list)\n",
    "    final_path    = merge_shards(shards, meta)\n",
    "    validate_outputs(final_path)\n",
    "\n",
    "defensive_metrics_collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb9e2fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/nba_api_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/nba_api_ingest.py\n",
    "# dags/nba_api_ingest.py\n",
    "\"\"\"\n",
    "Pulls roster + box‑score data from nba_api once per hour and writes Parquet\n",
    "partitions under data/new_processed/season=<YYYY-YY>/part.parquet.\n",
    "\n",
    "Why hourly?\n",
    "• The NBA Stats endpoints update within minutes after a game ends.\n",
    "• Hourly keeps your lake near‑real‑time without hammering the API.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys, pathlib\n",
    "\n",
    "# ── Use centralized config ─────────────────────────────────────────────────────\n",
    "from utils.nba_basics_config import PROJECT_ROOT, DATA_DIR\n",
    "from utils.incremental_utils import (\n",
    "    get_latest_ingested_date, \n",
    "    get_next_date_to_pull, \n",
    "    incremental_pull,\n",
    "    should_pull_incremental\n",
    ")\n",
    "\n",
    "# Allow `nba_basic_advanced_stats` imports\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"api\" / \"src\"))\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,      # explicit\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_api_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@hourly\",            # unified scheduling API (Airflow ≥ 2.4)\n",
    "    catchup=False,\n",
    "    default_args=default_args,\n",
    "    max_active_runs=1,             # avoid overlapping pulls\n",
    "    tags=[\"nba\", \"api\", \"ingest\"],\n",
    "    params={\n",
    "        \"start_year\": 2024,  # first season to pull\n",
    "        \"end_year\":   2025,  # last season to pull\n",
    "    },\n",
    ") as dag:\n",
    "\n",
    "    def pull_incremental(**context):\n",
    "        \"\"\"\n",
    "        Determine the next date to pull based on existing data and perform incremental pull.\n",
    "        Falls back to full season pull if no existing data is found.\n",
    "        \"\"\"\n",
    "        p = context[\"params\"]\n",
    "        sy = int(p[\"start_year\"])\n",
    "        ey = int(p[\"end_year\"])\n",
    "        \n",
    "        # For now, focus on the current season (2024-25)\n",
    "        season = f\"{sy}-{str(sy+1)[-2:]}\"\n",
    "        \n",
    "        print(f\"[pull_incremental] Checking for incremental pull for season {season}\")\n",
    "        \n",
    "        # Check if we should do incremental pull\n",
    "        if should_pull_incremental(DATA_DIR, season):\n",
    "            # Get the next date to pull\n",
    "            next_date = get_next_date_to_pull(DATA_DIR, season)\n",
    "            if next_date:\n",
    "                print(f\"[pull_incremental] Pulling incremental data for {next_date}\")\n",
    "                incremental_pull(\n",
    "                    data_dir=DATA_DIR,\n",
    "                    season=season,\n",
    "                    date_to_pull=next_date,\n",
    "                    workers=8,\n",
    "                    debug=False\n",
    "                )\n",
    "            else:\n",
    "                print(f\"[pull_incremental] No next date found, skipping\")\n",
    "        else:\n",
    "            # Fall back to full season pull\n",
    "            print(f\"[pull_incremental] No existing data found, doing full season pull\")\n",
    "            from eda.nba_basic_advanced_stats.main import main as pull_main\n",
    "            pull_main(\n",
    "                start_year=sy,\n",
    "                end_year=ey,\n",
    "                small_debug=True,\n",
    "                workers=8,\n",
    "                overwrite=False,\n",
    "                output_base=str(DATA_DIR),\n",
    "            )\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_incremental_data\",\n",
    "        python_callable=pull_incremental,\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a580bb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/nba_advanced_ingest.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/nba_advanced_ingest.py\n",
    "# dags/nba_advanced_ingest.py\n",
    "\"\"\"\n",
    "Daily scrape of Basketball‑Reference season‑level advanced metrics.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ── Use centralized config ─────────────────────────────────────────────────────\n",
    "from utils.nba_basics_config import PROJECT_ROOT, ADVANCED_METRICS_DIR\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"api\" / \"src\"))\n",
    "from eda.nba_basic_advanced_stats.scrape_utils import _season_advanced_df\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=1),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_advanced_ingest\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"advanced\", \"ingest\"],\n",
    "    params={\n",
    "        \"start_year\": 2024,  # first season to scrape\n",
    "        \"end_year\":   2025,  # last season to scrape\n",
    "    },\n",
    ") as dag:\n",
    "\n",
    "    def scrape_adv(**ctx):\n",
    "        \"\"\"\n",
    "        Loop through start_year..end_year, fetch each season's\n",
    "        advanced stats, and write to parquet.\n",
    "\n",
    "        - If a season fetch fails, log a big warning and skip it.\n",
    "        - After looping, list all missing seasons in a single warning.\n",
    "        - If _any_ missing season falls in its data-window (Nov–Jun),\n",
    "        raise an error so the DAG fails (data should exist).\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "\n",
    "        p = ctx[\"params\"]\n",
    "        start_year = int(p[\"start_year\"])\n",
    "        end_year   = int(p[\"end_year\"])\n",
    "\n",
    "        missing = []\n",
    "\n",
    "        # 1️⃣ Fetch each season\n",
    "        for y in range(start_year, end_year + 1):\n",
    "            season = f\"{y}-{str(y+1)[-2:]}\"\n",
    "            try:\n",
    "                df = _season_advanced_df(season)\n",
    "            except RuntimeError as err:\n",
    "                print(f\"⚠️  [WARNING] Unable to fetch advanced stats for {season}: {err}\")\n",
    "                missing.append(season)\n",
    "                continue\n",
    "\n",
    "            out_dir = ADVANCED_METRICS_DIR\n",
    "            out_dir.mkdir(parents=True, exist_ok=True)\n",
    "            df.to_parquet(out_dir / f\"advanced_{season}.parquet\", index=False)\n",
    "\n",
    "        # 2️⃣ Summary missing seasons\n",
    "        if missing:\n",
    "            print(\"\\n⚠️⚠️⚠️  Missing advanced data for seasons:\", \", \".join(missing), \"⚠️⚠️⚠️\\n\")\n",
    "\n",
    "            # 3️⃣ If it's currently Nov–Jun for any missing season, fail\n",
    "            now = datetime.now()\n",
    "            should_exist = []\n",
    "            for season in missing:\n",
    "                sy = int(season[:4])\n",
    "                if (now.year == sy and now.month >= 11) or \\\n",
    "                (now.year == sy + 1 and 1 <= now.month <= 6):\n",
    "                    should_exist.append(season)\n",
    "\n",
    "            if should_exist:\n",
    "                raise RuntimeError(\n",
    "                    f\"Advanced stats pages _should_ exist for: {', '.join(should_exist)} \"\n",
    "                    f\"(current month: {now.month}). Aborting DAG.\"\n",
    "                )\n",
    "\n",
    "    PythonOperator(\n",
    "        task_id=\"scrape_advanced_metrics\",\n",
    "        python_callable=scrape_adv,\n",
    "    ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7930b0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting api/src/airflow_project/dags/nba_data_loader.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile api/src/airflow_project/dags/nba_data_loader.py\n",
    "# dags/nba_data_loader.py\n",
    "\"\"\"\n",
    "Load processed NBA data into DuckDB for analysis.\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.sensors.external_task import ExternalTaskSensor\n",
    "from datetime import datetime, timedelta\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# ── Use centralized config ─────────────────────────────────────────────────────\n",
    "from utils.config import (\n",
    "    PROJECT_ROOT, DATA_DIR, INJURY_DIR, NBA_BASE_DATA_DIR\n",
    ")\n",
    "\n",
    "# Ensure our code can import the eda package\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"api\" / \"src\"))\n",
    "from eda.nba_basic_advanced_stats.data_utils import validate_data\n",
    "\n",
    "# Use centralized data root from config\n",
    "DATA_ROOT = DATA_DIR\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"data_eng\",\n",
    "    \"email\": [\"alerts@example.com\"],\n",
    "    \"email_on_failure\": True,\n",
    "    \"depends_on_past\": False,\n",
    "    \"retries\": 2,\n",
    "    \"retry_delay\": timedelta(minutes=5),\n",
    "    \"sla\": timedelta(hours=3),\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"nba_data_loader\",\n",
    "    start_date=datetime(2025, 7, 1),\n",
    "    schedule=\"@daily\",\n",
    "    catchup=False,\n",
    "    max_active_runs=1,\n",
    "    default_args=default_args,\n",
    "    tags=[\"nba\", \"loader\", \"duckdb\"],\n",
    "    params={\"season\": \"2024-25\"},\n",
    ") as dag:\n",
    "\n",
    "    # ─── sensors (one per upstream DAG) ────────────────────────────────\n",
    "    sensor_args = dict(\n",
    "        poke_interval=300,\n",
    "        mode=\"reschedule\",   # avoids tying up a worker slot\n",
    "    )\n",
    "    wait_api = ExternalTaskSensor(\n",
    "        task_id=\"wait_api_ingest\",\n",
    "        external_dag_id=\"nba_api_ingest\",\n",
    "        external_task_id=\"scrape_incremental_data\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_adv = ExternalTaskSensor(\n",
    "        task_id=\"wait_advanced_ingest\",\n",
    "        external_dag_id=\"nba_advanced_ingest\",\n",
    "        external_task_id=\"scrape_advanced_metrics\",\n",
    "        timeout=3600,\n",
    "        **sensor_args,\n",
    "    )\n",
    "    wait_injury = ExternalTaskSensor(\n",
    "        task_id=\"wait_injury_etl\",\n",
    "        external_dag_id=\"injury_etl\",\n",
    "        external_task_id=\"process_injury_data\",\n",
    "        timeout=7200,\n",
    "        poke_interval=600,\n",
    "        mode=\"reschedule\",\n",
    "    )\n",
    "\n",
    "    # ─── loader task ───────────────────────────────────────────────────\n",
    "    def load_to_duckdb(**ctx):\n",
    "        season = ctx[\"params\"][\"season\"]\n",
    "        # ▶ use centralized DATA_DIR for everything\n",
    "        db_path = DATA_DIR / \"nba_stats.duckdb\"\n",
    "        con = duckdb.connect(db_path)\n",
    "\n",
    "        sources = {\n",
    "            f\"player_{season}\": NBA_BASE_DATA_DIR / f\"season={season}/part.parquet\",\n",
    "            f\"advanced_{season}\": DATA_DIR / f\"new_processed/advanced_metrics/advanced_{season}.parquet\",\n",
    "            \"injury_master\": INJURY_DIR / \"injury_master.parquet\",\n",
    "        }\n",
    "\n",
    "        for alias, path in sources.items():\n",
    "            if path.exists():\n",
    "                if alias.startswith(\"player\"):\n",
    "                    df = pd.read_parquet(path)\n",
    "                    validate_data(df, name=alias, save_reports=True)\n",
    "                con.execute(\n",
    "                    f\"CREATE OR REPLACE TABLE {alias.replace('-', '_')} AS \"\n",
    "                    f\"SELECT * FROM read_parquet('{path}')\"\n",
    "                )\n",
    "\n",
    "        # materialised view – wildcard parquet scan is fine too\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE OR REPLACE VIEW v_player_full_{season.replace('-', '_')} AS\n",
    "            SELECT *\n",
    "            FROM player_{season.replace('-', '_')} p\n",
    "            LEFT JOIN advanced_{season.replace('-', '_')} a USING(player, season)\n",
    "            LEFT JOIN injury_master i USING(player, season)\n",
    "        \"\"\")\n",
    "        con.close()\n",
    "\n",
    "    loader = PythonOperator(\n",
    "        task_id=\"validate_and_load\",\n",
    "        python_callable=load_to_duckdb,\n",
    "    )\n",
    "\n",
    "    [wait_api, wait_adv, wait_injury] >> loader \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
